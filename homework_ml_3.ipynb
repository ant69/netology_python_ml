{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be87c42d",
   "metadata": {},
   "source": [
    "# Задание\n",
    "## Вопросы по заданию\n",
    "### Преподаватель: Даниил Корбут, Наталья Баданина, Александр Миленькин\n",
    "\n",
    "# Задание\n",
    "\n",
    "# Цель: \n",
    "изучить применение методов оптимизации для решения задачи классификации\n",
    "\n",
    "# Описание задания:\n",
    "В домашнем задании необходимо применить полученные знания в теории оптимизации и машинном обучении для реализации логистической регрессии.\n",
    "\n",
    "## Этапы работы:**\n",
    "\n",
    "### Загрузите данные. \n",
    "- Используйте датасет с ирисами (https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_iris.html). Его можно загрузить непосредственно из библиотеки Sklearn. В данных оставьте только 2 класса: Iris Versicolor, Iris Virginica.\n",
    "- Самостоятельно реализуйте логистическую регрессию, без использования метода LogisticRegression из библиотеки. Можете использовать библиотеки pandas, numpy, math для реализации. Оформите в виде функции. *Оформите в виде класса с методами.\n",
    "- Реализуйте метод градиентного спуска. Обучите логистическую регрессию этим методом. Выберете и посчитайте метрику качества. Метрика должна быть одинакова для всех пунктов домашнего задания. Для упрощения сравнения выберете только одну метрику.\n",
    "- Повторите п. 3 для метода скользящего среднего (Root Mean Square Propagation, RMSProp).\n",
    "- Повторите п. 3 для ускоренного по Нестерову метода адаптивной оценки моментов (Nesterov–accelerated Adaptive Moment Estimation, Nadam).\n",
    "- Сравните значение метрик для реализованных методов оптимизации. Можно оформить в виде таблицы вида |метод|метрика|время работы| (время работы опционально). Напишите вывод.\n",
    "\n",
    "\n",
    "Для лучшего понимания темы и упрощения реализации можете обратиться к статье:\n",
    "https://habr.com/en/post/318970/\n",
    "\n",
    "Для получение зачета по этому домашнему заданию, минимально, должно быть реализовано обучение логистической регрессии и градиентный спуск.\n",
    "\n",
    "### Результат: \n",
    "получены навыки реализации методов оптимизации в задаче бинарной классификации. Пройденные методы оптимизации используются и в нейросетях.\n",
    "\n",
    "### Форма выполнения: \n",
    "ссылка на Jupyter Notebook, загруженный на GitHub; ссылка на Google Colab; файл с расширением .ipynb.\n",
    "\n",
    "### Инструменты: \n",
    "Jupyter Notebook/Google Colab; GitHub.\n",
    "\n",
    "*Рекомендации к выполнению:\n",
    "- Текст оформляйте в отдельной ячейке Jupyter Notebook/Google Colab в формате markdown.\n",
    "- У графиков должен быть заголовок, подписи осей, легенда (опционально). Делайте графики бОльшего размера, чем стандартный вывод, чтобы увеличить читаемость.\n",
    "- Убедитесь, что по ссылкам есть доступ на чтение/просмотр.\n",
    "- Убедитесь, что все ячейки в работе выполнены и можно увидеть их вывод без повторного запуска."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed740ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################\n",
    "# Используйте датасет с ирисами (https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_iris.html). \n",
    "# Его можно загрузить непосредственно из библиотеки Sklearn. В данных оставьте только 2 класса: Iris Versicolor, Iris Virginica.\n",
    "##############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33100b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1115b66d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _iris_dataset:\n",
      "\n",
      "Iris plants dataset\n",
      "--------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    :Number of Instances: 150 (50 in each of three classes)\n",
      "    :Number of Attributes: 4 numeric, predictive attributes and the class\n",
      "    :Attribute Information:\n",
      "        - sepal length in cm\n",
      "        - sepal width in cm\n",
      "        - petal length in cm\n",
      "        - petal width in cm\n",
      "        - class:\n",
      "                - Iris-Setosa\n",
      "                - Iris-Versicolour\n",
      "                - Iris-Virginica\n",
      "                \n",
      "    :Summary Statistics:\n",
      "\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "                    Min  Max   Mean    SD   Class Correlation\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "    sepal length:   4.3  7.9   5.84   0.83    0.7826\n",
      "    sepal width:    2.0  4.4   3.05   0.43   -0.4194\n",
      "    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\n",
      "    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "    :Class Distribution: 33.3% for each of 3 classes.\n",
      "    :Creator: R.A. Fisher\n",
      "    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n",
      "    :Date: July, 1988\n",
      "\n",
      "The famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\n",
      "from Fisher's paper. Note that it's the same as in R, but not as in the UCI\n",
      "Machine Learning Repository, which has two wrong data points.\n",
      "\n",
      "This is perhaps the best known database to be found in the\n",
      "pattern recognition literature.  Fisher's paper is a classic in the field and\n",
      "is referenced frequently to this day.  (See Duda & Hart, for example.)  The\n",
      "data set contains 3 classes of 50 instances each, where each class refers to a\n",
      "type of iris plant.  One class is linearly separable from the other 2; the\n",
      "latter are NOT linearly separable from each other.\n",
      "\n",
      ".. topic:: References\n",
      "\n",
      "   - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\n",
      "     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\n",
      "     Mathematical Statistics\" (John Wiley, NY, 1950).\n",
      "   - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\n",
      "     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\n",
      "   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\n",
      "     Structure and Classification Rule for Recognition in Partially Exposed\n",
      "     Environments\".  IEEE Transactions on Pattern Analysis and Machine\n",
      "     Intelligence, Vol. PAMI-2, No. 1, 67-71.\n",
      "   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\n",
      "     on Information Theory, May 1972, 431-433.\n",
      "   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\n",
      "     conceptual clustering system finds 3 classes in the data.\n",
      "   - Many, many more ...\n"
     ]
    }
   ],
   "source": [
    "# Загружаем датасет и сразу же смотрим, что загружено\n",
    "iris_dataset = load_iris()\n",
    "print(iris_dataset.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9035a44f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "      <th>variety</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>7.0</td>\n",
       "      <td>3.2</td>\n",
       "      <td>4.7</td>\n",
       "      <td>1.4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>6.4</td>\n",
       "      <td>3.2</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>6.9</td>\n",
       "      <td>3.1</td>\n",
       "      <td>4.9</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>5.5</td>\n",
       "      <td>2.3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>6.5</td>\n",
       "      <td>2.8</td>\n",
       "      <td>4.6</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
       "50                7.0               3.2                4.7               1.4   \n",
       "51                6.4               3.2                4.5               1.5   \n",
       "52                6.9               3.1                4.9               1.5   \n",
       "53                5.5               2.3                4.0               1.3   \n",
       "54                6.5               2.8                4.6               1.5   \n",
       "\n",
       "    variety  \n",
       "50        1  \n",
       "51        1  \n",
       "52        1  \n",
       "53        1  \n",
       "54        1  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##############################\n",
    "# В данных оставьте только 2 класса: Iris Versicolor, Iris Virginica.\n",
    "##############################\n",
    "\n",
    "# загружаем данные в датафрейм\n",
    "iris_data = pd.DataFrame(iris_dataset.data, columns=iris_dataset.feature_names)\n",
    "#print(iris_data.describe())\n",
    "# загружаем также целевые значения\n",
    "variety = iris_dataset.target\n",
    "# print(variety)\n",
    "# добавляем колонку с целевыми значениями в общий датафрейм\n",
    "iris_data['variety'] = pd.Series(variety)\n",
    "#print(iris_data)\n",
    "\n",
    "# создаем копию датафрейма, в котором оставлены только два класса. \n",
    "# Исходим из того, что Iris Versicolor = 1, Iris Virginica = 2\n",
    "iris_filtered = iris_data.loc[(iris_data['variety'] > 0)].copy()\n",
    "iris_filtered.head()\n",
    "\n",
    "# поскольку в колонке с целевым значением у нас остались только 1 и 2, будет правильно провести замену 2 на 0\n",
    "# В этом случае в нашем наборе данных будем считать, что Iris Virginica будет с 0, а Iris Versicolor сохранит 1\n",
    "iris_filtered.loc[(iris_filtered.variety == 2), 'variety'] = 0\n",
    "iris_filtered.head()\n",
    "\n",
    "X = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa152a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Самостоятельно реализуйте логистическую регрессию, без использования метода LogisticRegression из библиотеки. \n",
    "# Можете использовать библиотеки pandas, numpy, math для реализации. Оформите в виде функции.*Оформите в виде класса с методами.\n",
    "\n",
    "# Формулировка этой части задания кажется мне некорректной. Нам здесь надо реализовать не логистическую регрессию, \n",
    "# а именно функцию, которая возвращает некоторые значения для конкретного шага оптимизации. Или нет?\n",
    "# И уже набор этих функций будет вызываться конкретной реализацией?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "df23cde6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.60279641e-09],\n",
       "       [9.99997740e-01],\n",
       "       [9.93307149e-01]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# В основу реализации положим материалы этой https://habr.com/ru/company/ods/blog/323890/#2-logisticheskaya-regressiya\n",
    "# и других публикаций про логистическую регрессию\n",
    "\n",
    "# Вначале определяем функцию сигмоиды, которая должна вернуть вероятность принадлежности текущего объекта к 1 или 2 классу\n",
    "# при текущих значениях коэффициентов логистической функции.\n",
    "# xi – вектор признаков примера (вместе с единицей);\n",
    "# wi – веса в линейной модели (вместе с нулевым смещением w_0)\n",
    "def f_sigmoid(xi, wi):\n",
    "    yi = np.dot(xi, wi)\n",
    "    return 1/(1 + np.exp(-yi))  \n",
    "\n",
    "# блок тестирования - для себя\n",
    "x_ = [[1, -3, 4, 2, 5],[1, -3, -4, 2, 5],[1, 3, 4, 2, 5]] \n",
    "w_ = [[0, 4, -4, 2, 1]]\n",
    "f_sigmoid(x_, np.transpose(w_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9fe1f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Определим функцию потерь. \n",
    "def f_loss(h, y):\n",
    "    return (-y * np.log(h) - (1 - y) * np.log(1 - h)).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "6777efaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "X = iris_filtered.iloc[:,[0,1,2,3]]\n",
    "Y = iris_filtered.variety.tolist()\n",
    "#print(Y)\n",
    "#intercept = np.ones((x.shape[0], 1))\n",
    "#print(np.zeros(x.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea367c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "43d64fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Объединяем в один класс ранее определенные функции, а также добавляем дополнительные методы\n",
    "# Постараемся в этом классе определить все заданные по условию задачи способы поиска решения \n",
    "# (Метод градиентного спуска, RMSProp, Nadam)\n",
    "\n",
    "class NetologiaLogisticRegression:\n",
    "    # Метод __init__ - стандартный конструктор класса.\n",
    "    def __init__(self, x, y, method):      \n",
    "        # Создаем колонку из единиц с количеством строк, совпадающим с количеством строк в выборке x\n",
    "        self.intercept = np.ones((x.shape[0], 1))  \n",
    "        # Добавляем эту колонку в выборку x\n",
    "        self.x = np.concatenate((self.intercept, x), axis=1)\n",
    "        # Создаем нулевой вектор с начальными коэффициентами\n",
    "        self.coefs = np.zeros(self.x.shape[1])\n",
    "        # Сохраняем значения целевых значений\n",
    "        self.y = y\n",
    "        # Сохраняем имя метода для выполнения поиска решения\n",
    "        # Допустимые значения: GradDesc, RMSProp, Nadam\n",
    "        self.method = method\n",
    "        # Начальная инициализация показателя функции потерь\n",
    "        self.loss = 9999\n",
    "        \n",
    "    # Метод f_sigmoid для вычисления сигмоиды\n",
    "    def f_sigmoid(self, x, coefs):\n",
    "        y_ = np.dot(x, coefs)\n",
    "        return 1/(1 + np.exp(-y_))\n",
    "    \n",
    "    # Метод f_loss для вычисления потерь\n",
    "    def f_loss(self, h, y):\n",
    "        return (-y * np.log(h) - (1 - y) * np.log(1 - h)).mean()\n",
    "    \n",
    "    # Метод градиентного спуска\n",
    "    def grad_desc(self, XX, h, y):\n",
    "        return np.dot(XX.T, (h - y)) / XX.shape[0]\n",
    "\n",
    "    # Метод RMSProp (заготовка)\n",
    "    def rmsprop(self, XX, h, y):\n",
    "        return 0\n",
    "\n",
    "    # Метод Nadam (заготовка)\n",
    "    def nadam(self, XX, h, y):\n",
    "        return 0\n",
    "\n",
    "    # Метод для обучения модели. \n",
    "    # \n",
    "    def fit(self, lr, cycles):\n",
    "        for i in range(cycles):\n",
    "            # print(self.coefs)            \n",
    "            calc_sigmoid = self.f_sigmoid(self.x, self.coefs)            \n",
    "            \n",
    "            # Проверка для себя результатов выполнения функции вычисления потерь\n",
    "            self.loss = self.f_loss(calc_sigmoid, pd.Series(self.y))\n",
    "            #print(loss)   \n",
    "            if self.method == 'GradDesc':\n",
    "                dW = self.grad_desc(self.x, calc_sigmoid, self.y)\n",
    "            \n",
    "                # Корректируем весовые коэффициенты\n",
    "                self.coefs -= lr * dW\n",
    "                #print(self.coefs)\n",
    "            else:\n",
    "                continue\n",
    "            \n",
    "        return print('Модель обучена')\n",
    "    \n",
    "    # Метод для предсказания класса. Его можно использовать после того, как модель уже обучена и коэффициенты подобраны\n",
    "    # Параметр treshold определяет границу отнесения записей к тому или иному классу. 0.5 - пополам\n",
    "    def predict(self, x_new , treshold):\n",
    "        intercept_new = np.ones((x_new.shape[0], 1))\n",
    "        x_new = np.concatenate((intercept_new, x_new), axis=1)\n",
    "        result = self.f_sigmoid(x_new, self.coefs)\n",
    "        result = result >= treshold\n",
    "        y_pred = np.zeros(result.shape[0])\n",
    "        for i in range(len(y_pred)):\n",
    "            if result[i] == True: \n",
    "                y_pred[i] = 1\n",
    "            else:\n",
    "                continue\n",
    "                \n",
    "\n",
    "                return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "f910398f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Модель обучена\n",
      "Точность модели:  0.97\n",
      "Значение функции потерь:  0.10543814064361162\n",
      "Wall time: 10.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Реализуйте метод градиентного спуска. Обучите логистическую регрессию этим методом. Выберете и посчитайте метрику качества. \n",
    "# Метрика должна быть одинакова для всех пунктов домашнего задания. Для упрощения сравнения выберете только одну метрику.\n",
    "\n",
    "GradRegr = NetologiaLogisticRegression(X, Y, 'GradDesc')\n",
    "\n",
    "# 0.1 - шаговый коэффициент, 5000 - количество итераций\n",
    "GradRegr.fit(0.1, 5000)\n",
    "\n",
    "y_predict = GradRegr.predict(X, 0.45)\n",
    "\n",
    "print(f'Точность модели:  {sum(y_predict == Y) / len(Y)}')\n",
    "print(f'Значение функции потерь:  {GradRegr.loss}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "cd745d70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Модель обучена\n",
      "Точность предсказания:  1.0\n",
      "Значение функции потерь:  0.12211368661362562\n",
      "Wall time: 10.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Попробуем проверить эффективность предсказания, разделив выборку на тестовые данные\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Исходный датасет относительно невелик (100 строк), поэтому для тестовых данных оставляем 20% строк (20)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.20)\n",
    "\n",
    "GradRegr80 = NetologiaLogisticRegression(X_train, y_train, 'GradDesc')\n",
    "\n",
    "GradRegr80.fit(0.1, 5000)\n",
    "\n",
    "y_predict20 = GradRegr.predict(X_test, 0.5)\n",
    "\n",
    "print(f'Точность предсказания:  {sum(y_predict20 == y_test) / len(y_test)}')\n",
    "print(f'Значение функции потерь:  {GradRegr80.loss}')\n",
    "\n",
    "# При повторных запусках этого блока точность предсказания \"плавает\" от 0.9 до 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48624845",
   "metadata": {},
   "source": [
    "\n",
    "Повторите п. 3 для метода скользящего среднего (Root Mean Square Propagation, RMSProp).\n",
    "\n",
    "Повторите п. 3 для ускоренного по Нестерову метода адаптивной оценки моментов (Nesterov–accelerated Adaptive Moment Estimation, Nadam).\n",
    "Сравните значение метрик для реализованных методов оптимизации. Можно оформить в виде таблицы вида |метод|метрика|время работы| (время работы опционально). Напишите вывод."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1385453",
   "metadata": {},
   "outputs": [],
   "source": [
    "# С методами RMSP и Nadam не справился... Задание показалось очень сложным, сидел с тем, что успел сделать, два дня."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
